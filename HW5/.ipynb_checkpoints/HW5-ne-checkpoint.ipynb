{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isdir\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.4. Obtain the activities of daily life dataset from the UC Irvine machine learning\n",
    "website (https://archive.ics.uci.edu/ml/datasets/Dataset+for+ADL+Recognition+with+Wrist-worn+Accelerom data provided by Barbara Bruno, Fulvio Mastrogiovanni and Antonio Sgor-\n",
    "bissa).\n",
    "\n",
    "\n",
    "(a) Build a classifier that classifies sequences into one of the 14 activities pro-\n",
    "vided. To make features, you should vector quantize, then use a histogram of cluster centers (as described in the subsection; this gives a pretty ex- plicit set of steps to follow). You will find it helpful to use hierarchical k-means to vector quantize. You may use whatever multi-class classifier you wish, though I’d start with R’s decision forest, because it’s easy to use and effective. You should report (a) the total error rate and (b) the class confusion matrix of your classifier.\n",
    "\n",
    "\n",
    "(b) Now see if you can improve your classifier by (a) modifying the number of cluster centers in your hierarchical k-means and (b) modifying the size of the fixed length samples that you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(X, y, test_data):\n",
    "        for num, data in enumerate(test_data):\n",
    "            y[num] = label.index(data[1])\n",
    "            for j in kmeans.predict(data[0]):\n",
    "                X[num, j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input data\n",
    "dataset = \"HMP_Dataset/\"\n",
    "\n",
    "dir_name = [x for x in listdir(dataset) if \"MODEL\" not in x and isdir(dataset+x)]\n",
    "data_dict = {}\n",
    "\n",
    "for d_name in dir_name:\n",
    "    total_files = []\n",
    "    for f_name in listdir(dataset+d_name):\n",
    "        cur_file = np.loadtxt(dataset+d_name+'/'+f_name,dtype=int)\n",
    "        total_files.append(cur_file)\n",
    "    data_dict[d_name] = total_files\n",
    "label = list(data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_len = 25\n",
    "n_cluster = 14\n",
    "n_estimator=100\n",
    "depth=10\n",
    "testsize = 0.2\n",
    "#====================================\n",
    "#=^^^^Parameter^^^^^^^^^^^^^^========\n",
    "#====================================\n",
    "test_slice = []\n",
    "test_whole = []\n",
    "train_slice = []\n",
    "train_whole = []\n",
    "total = []\n",
    "testal = []\n",
    "train_y = []\n",
    "for k, arr in data_dict.items():\n",
    "    test_y = []\n",
    "    test_slice = []\n",
    "    for cur in arr:\n",
    "        whole = []\n",
    "        tt = []\n",
    "        lb = []\n",
    "        for i in range(0,cur.shape[0]-seg_len):\n",
    "            temp = cur[i:i+seg_len].reshape(-1)\n",
    "            tt.append(temp)\n",
    "            lb.append(label.index(k))\n",
    "            whole.append(temp)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(tt,lb,test_size=testsize)\n",
    "        train_slice += X_train\n",
    "        test_slice += X_test\n",
    "        test_y += y_test\n",
    "        train_y += y_train\n",
    "        testal.append((np.asarray(X_test),k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=n_cluster, random_state=np.random.randint(491212)).fit(train_slice)\n",
    "clf = RandomForestClassifier(n_estimators=n_estimator, max_depth=depth, random_state=87412)\n",
    "\n",
    "X_test = np.zeros((len(testal), n_cluster))\n",
    "y_test = np.zeros(len(testal), dtype='int')\n",
    "\n",
    "convert(X_test, y_test, testal)\n",
    "clf.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.score(X_test, y_test))\n",
    "confusion_matrix(y_test, clf.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
